{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f2c6665-b1de-4ee2-9d09-6fe26dd16b87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import *\n",
    "from pyspark.sql.functions import col\n",
    "import  sys\n",
    "import os\n",
    "\n",
    "\n",
    "path = '/Workspace/Users/joshilopez1609@gmail.com/glo-de-chll'\n",
    "\n",
    "sys.path.append(os.path.abspath(path))\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adbf0544-9d7f-4d8a-b505-3a6269b784a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#connection with ADLS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b5ee690-0f99-4d4b-b4be-1ed9a47359e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# For testing purposes\n",
    "dbutils.fs.ls(\"abfss://raw@storage2de.dfs.core.windows.net/\")\n",
    "dbutils.fs.ls(\"abfss://backups@storage2de.dfs.core.windows.net/\")\n",
    "dbutils.fs.ls(\"abfss://logs@storage2de.dfs.core.windows.net/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77c3e26e-cd03-4585-953b-75726f8a2e93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"abfss://raw@storage2de.dfs.core.windows.net/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e6d176c-e4f7-4752-9ba6-b34b957f9be0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Assigning the schema as needed\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "hired_employees_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True), \n",
    "    StructField(\"name\", StringType(), True),  \n",
    "    StructField(\"datetime\", StringType(), True),  \n",
    "    StructField(\"department_id\", IntegerType(), True),  \n",
    "    StructField(\"job_id\", IntegerType(), True)  \n",
    "])\n",
    "\n",
    "departments_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),  \n",
    "    StructField(\"department\", StringType(), True),  \n",
    "])\n",
    "\n",
    "\n",
    "jobs_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True), \n",
    "    StructField(\"job\", StringType(), True)  \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd51af59-e7d2-4b80-9059-e00552f358db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hired_employees_df = spark.read.csv(\"abfss://raw@storage2de.dfs.core.windows.net/hired_employees.csv\", header=False, schema=hired_employees_schema)\n",
    "departments_df = spark.read.csv(\"abfss://raw@storage2de.dfs.core.windows.net/departments.csv\", header=False, schema=departments_schema)\n",
    "jobs_df = spark.read.csv(\"abfss://raw@storage2de.dfs.core.windows.net/jobs.csv\", header=False,schema=jobs_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ad870e2-a92f-4e98-a3a7-f68fa3885868",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "invalid_hired = hired_employees_df.filter(\n",
    "    col(\"id\").isNull() | col(\"name\").isNull() | col(\"datetime\").isNull() |\n",
    "    col(\"department_id\").isNull() | col(\"job_id\").isNull()\n",
    ")\n",
    "invalid_hired.write.mode(\"append\").csv(\"abfss://logs@storage2de.dfs.core.windows.net/\")\n",
    "valid_hired = hired_employees_df.na.drop()\n",
    "\n",
    "invalid_departments = departments_df.filter(\n",
    "    col(\"id\").isNull() | col(\"department\").isNull()\n",
    ")\n",
    "invalid_departments.write.mode(\"append\").csv(\"abfss://logs@storage2de.dfs.core.windows.net/\")\n",
    "valid_departments = departments_df.na.drop()\n",
    "\n",
    "invalid_jobs = jobs_df.filter(\n",
    "    col(\"id\").isNull() | col(\"job\").isNull()\n",
    ")\n",
    "invalid_jobs.write.mode(\"append\").csv(\"abfss://logs@storage2de.dfs.core.windows.net/\")\n",
    "valid_jobs = jobs_df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25970b6a-887f-4017-b8ba-d84259c4a770",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Call the function to write in the database\n",
    "\n",
    "write_to_sql(valid_departments, \"departments\", jdbc_url, connection_properties)\n",
    "write_to_sql(valid_jobs, \"jobs\", jdbc_url, connection_properties)\n",
    "write_to_sql(valid_hired, \"hired_employees\", jdbc_url, connection_properties)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "main",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
